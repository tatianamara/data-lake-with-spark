# Data Lake Project

## Table of Contents

- [Data Lake Project](#data-lake-project)
  - [Table of Contents](#table-of-contents)
  - [Introduction](#introduction)
    - [Objectives](#objectives)
    - [Star Schema Model](#star-schema-model)
  - [What this repo contains](#what-this-repo-contains)
      - [ETL.py](#etlpy)
      - [dwg.cfg](#dwgcfg)
      - [Star_schema_model.png](#star_schema_modelpng)
  - [Get Started](#get-started)
      - [Run the etl.py file to load data from S3, process the data into analytics tables using Spark, and load them back into S3.](#run-the-etlpy-file-to-load-data-from-s3-process-the-data-into-analytics-tables-using-spark-and-load-them-back-into-s3)
    - [Prerequisites](#prerequisites)
  - [Project Datasets](#project-datasets)
    - [Song Dataset](#song-dataset)
    - [Log Dataset](#log-dataset)

## Introduction  
A music streaming startup, Sparkify, has grown their user base and song database even more and want to move their data warehouse to a data lake. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

As their data engineer, you are tasked with building an ETL pipeline that extracts their data from S3, processes them using Spark, and loads the data back into S3 as a set of dimensional tables. This will allow their analytics team to continue finding insights in what songs their users are listening to.

You'll be able to test your database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

### Objectives  

- Apply what I've learned on Spark and data lakes to build an ETL pipeline for a data lake hosted on S3.

- Load data from S3, process the data into analytics tables using Spark, and load them back into S3. 

- Deploy this Spark process on a cluster using AWS.

### Star Schema Model  
![Star Schema Model](https://github.com/tatianamara/data-lake-with-spark.git/blob/main/star_schema_model.png)

- The fact table `songplays` stores the records in log data associated with song plays i.e. records with page.

- The dimension table `users` stores the users in the app.

- The dimension table `songs` stores the songs in the music database.

- The dimension table `artists` stores the artists the in music database.

- The dimension table `time` stores the timestamps of records in songplays broken down into specific units.

## What this repo contains
```
etl.py
dwh.cfg
star_schema_model.png
```

#### ETL.py
This script contains the code to load data from S3, process the data into analytics tables using Spark, and load them back into S3.

#### dwg.cfg
Configuration file to add the redshift cluster data and credentials to be accessible by code.

#### Star_schema_model.png
The star schema model used to create the tables for this project.

## Get Started  

`git clone https://github.com/tatianamara/data-lake-with-spark.git`

#### Run the etl.py file to load data from S3, process the data into analytics tables using Spark, and load them back into S3.  
`python3 etl.py`

### Prerequisites

- Python3 installed (you can download [here](https://www.python.org/downloads/))
- Populate the dwh.cfg file with the cluster and role data with the necessary permissions.

## Project Datasets  

### Song Dataset  
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. 
The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

File path in s3: `s3://udacity-dend/song_data`  

```
song_data/A/B/C/TRABCEI128F424C983.json  
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

### Log Dataset  

The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.
The log files in the dataset are partitioned by year and month. For example, here are filepaths to two files in this dataset.

File path in s3: `s3://udacity-dend/log_data`  
Log data json path: `s3://udacity-dend/log_json_path.json`  

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```





